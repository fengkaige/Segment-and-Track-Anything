{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengkaige/miniconda3/envs/segmentAndTrackAnything_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from SegTracker import SegTracker\n",
    "from model_args import aot_args,sam_args,segtracker_args\n",
    "from PIL import Image\n",
    "from aot_tracker import _palette\n",
    "import numpy as np\n",
    "import torch\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import binary_dilation\n",
    "import gc\n",
    "\n",
    "def create_output_dir(output_dir):\n",
    "    \"\"\"\n",
    "    创建给定路径的目录，如果它尚不存在的话。\n",
    "\n",
    "    参数:\n",
    "    output_dir (str): 待创建或检查的输出目录路径\n",
    "\n",
    "    返回:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"创建目录: {output_dir}\")\n",
    "    else:\n",
    "        # print(f\"目录 {output_dir} 已经存在\")\n",
    "        pass\n",
    "\n",
    "def save_prediction(pred_mask,output_dir,file_name):\n",
    "    create_output_dir(output_dir)\n",
    "    save_mask = Image.fromarray(pred_mask.astype(np.uint8))\n",
    "    save_mask = save_mask.convert(mode='P')\n",
    "    save_mask.putpalette(_palette)\n",
    "    save_mask.save(os.path.join(output_dir,file_name))\n",
    "def colorize_mask(pred_mask):\n",
    "    save_mask = Image.fromarray(pred_mask.astype(np.uint8))\n",
    "    save_mask = save_mask.convert(mode='P')\n",
    "    save_mask.putpalette(_palette)\n",
    "    save_mask = save_mask.convert(mode='RGB')\n",
    "    return np.array(save_mask)\n",
    "def draw_mask(img, mask, alpha=0.7, id_countour=False):\n",
    "    img_mask = np.zeros_like(img)\n",
    "    img_mask = img\n",
    "    if id_countour:\n",
    "        # very slow ~ 1s per image\n",
    "        obj_ids = np.unique(mask)\n",
    "        obj_ids = obj_ids[obj_ids!=0]\n",
    "\n",
    "        for id in obj_ids:\n",
    "            # Overlay color on  binary mask\n",
    "            if id <= 255:\n",
    "                color = _palette[id*3:id*3+3]\n",
    "            else:\n",
    "                color = [0,0,0]\n",
    "            foreground = img * (1-alpha) + np.ones_like(img) * alpha * np.array(color)\n",
    "            binary_mask = (mask == id)\n",
    "\n",
    "            # Compose image\n",
    "            img_mask[binary_mask] = foreground[binary_mask]\n",
    "\n",
    "            countours = binary_dilation(binary_mask,iterations=1) ^ binary_mask\n",
    "            img_mask[countours, :] = 0\n",
    "    else:\n",
    "        binary_mask = (mask!=0)\n",
    "        countours = binary_dilation(binary_mask,iterations=1) ^ binary_mask\n",
    "        foreground = img*(1-alpha)+colorize_mask(mask)*alpha\n",
    "        img_mask[binary_mask] = foreground[binary_mask]\n",
    "        img_mask[countours,:] = 0\n",
    "\n",
    "    return img_mask.astype(img.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = 'cars'\n",
    "# video_name = '240516-02'\n",
    "# video_name = '240529_1'\n",
    "# video_name = \"423937009-1-208\"\n",
    "# video_name = \"cell\"\n",
    "io_args = {\n",
    "    'input_video': f'./assets/{video_name}.mp4',\n",
    "    'output_mask_dir': f'./assets/{video_name}_masks_with_vit_matte', # save pred masks\n",
    "    'output_video': f'./assets/{video_name}_seg.mp4', # mask+frame vizualization, mp4 or avi, else the same as input video\n",
    "    'output_gif': f'./assets/{video_name}_seg.gif', # mask visualization\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Grounding-DINO and SAM on the First Frame for Good Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init vit matte\n",
    "import sys\n",
    "import os\n",
    "\n",
    "matte_anything_path = \"/home/fengkaige/codespace/Matte-Anything/\"\n",
    "os.chdir(matte_anything_path)\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "# from matte_anything import init_vitmatte\n",
    "\n",
    "\n",
    "from detectron2.config import LazyConfig, instantiate\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "\n",
    "def init_vitmatte(model_type):\n",
    "    \"\"\"\n",
    "    Initialize the vitmatte with model_type in ['vit_s', 'vit_b']\n",
    "    \"\"\"\n",
    "    cfg = LazyConfig.load(vitmatte_config[model_type])\n",
    "    vitmatte = instantiate(cfg.model)\n",
    "    vitmatte.to(device)\n",
    "    vitmatte.eval()\n",
    "    DetectionCheckpointer(vitmatte).load(vitmatte_models[model_type])\n",
    "\n",
    "    return vitmatte\n",
    "\n",
    "\n",
    "def generate_trimap(mask, erode_kernel_size=10, dilate_kernel_size=10):\n",
    "    erode_kernel = np.ones((erode_kernel_size, erode_kernel_size), np.uint8)\n",
    "    dilate_kernel = np.ones((dilate_kernel_size, dilate_kernel_size), np.uint8)\n",
    "    eroded = cv2.erode(mask, erode_kernel, iterations=5)\n",
    "    dilated = cv2.dilate(mask, dilate_kernel, iterations=5)\n",
    "    trimap = np.zeros_like(mask)\n",
    "    trimap[dilated == 255] = 128\n",
    "    trimap[eroded == 255] = 255\n",
    "    return trimap\n",
    "\n",
    "\n",
    "def generate_checkerboard_image(height, width, num_squares):\n",
    "    num_squares_h = num_squares\n",
    "    square_size_h = height // num_squares_h\n",
    "    square_size_w = square_size_h\n",
    "    num_squares_w = width // square_size_w\n",
    "\n",
    "    new_height = num_squares_h * square_size_h\n",
    "    new_width = num_squares_w * square_size_w\n",
    "    image = np.zeros((new_height, new_width), dtype=np.uint8)\n",
    "\n",
    "    for i in range(num_squares_h):\n",
    "        for j in range(num_squares_w):\n",
    "            start_x = j * square_size_w\n",
    "            start_y = i * square_size_h\n",
    "            color = 255 if (i + j) % 2 == 0 else 200\n",
    "            image[\n",
    "                start_y : start_y + square_size_h, start_x : start_x + square_size_w\n",
    "            ] = color\n",
    "\n",
    "    image = cv2.resize(image, (width, height))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def run_vit_matte(input_x, masks):\n",
    "\n",
    "    ### TODO !!!!!!! mask -> trimap\n",
    "    \"\"\"\n",
    "    masks\n",
    "        shape : (1, 1, 544, 890)\n",
    "        dtype : bool\n",
    "    \"\"\"\n",
    "    mask = masks.astype(np.uint8) * 255\n",
    "    # mask = masks\n",
    "    erode_kernel_size = 10\n",
    "    dilate_kernel_size = 10\n",
    "    trimap = generate_trimap(mask, erode_kernel_size, dilate_kernel_size).astype(\n",
    "        np.float32\n",
    "    )\n",
    "    # return trimap\n",
    "    trimap[trimap == 128] = 0.5\n",
    "    trimap[trimap == 255] = 1\n",
    "\n",
    "    # Vit Matte调用\n",
    "    \"\"\"\n",
    "        input_x\n",
    "            shape : (544, 890, 3)\n",
    "            dtype : uint8\n",
    "            type : numpy.ndarray\n",
    "        trimap\n",
    "            shape : (544, 890)\n",
    "            dtype : float32\n",
    "            max : 0.5\n",
    "            min : 0.0\n",
    "            type : numpy.ndarray\n",
    "    \"\"\"\n",
    "    input = {\n",
    "        \"image\": torch.from_numpy(input_x).permute(2, 0, 1).unsqueeze(0)\n",
    "        / 255,  # norm to [0, 1]\n",
    "        \"trimap\": torch.from_numpy(trimap).unsqueeze(0).unsqueeze(0),\n",
    "    }\n",
    "    # import pdb; pdb.set_trace()\n",
    "    torch.cuda.empty_cache()\n",
    "    alpha = vitmatte(input)[\"phas\"].flatten(0, 2)\n",
    "    alpha = alpha.detach().cpu().numpy()\n",
    "\n",
    "    return alpha\n",
    "\n",
    "    # # get a green background\n",
    "    # background = generate_checkerboard_image(input_x.shape[0], input_x.shape[1], 8)\n",
    "\n",
    "    # # calculate foreground with alpha blending\n",
    "    # foreground_alpha = input_x * np.expand_dims(alpha, axis=2).repeat(3,2)/255 + background * (1 - np.expand_dims(alpha, axis=2).repeat(3,2))/255\n",
    "\n",
    "    # return foreground_alpha\n",
    "    # background = generate_checkerboard_image(input_x.shape[0], input_x.shape[1], 8)\n",
    "\n",
    "    # # calculate foreground with mask\n",
    "    # foreground_mask = input_x * np.expand_dims(mask/255, axis=2).repeat(3,2)/255 + background * (1 - np.expand_dims(mask/255, axis=2).repeat(3,2))/255\n",
    "    # return foreground_mask\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "vitmatte_models = {\n",
    "    \"vit_b\": \"./pretrained/ViTMatte_B_DIS.pth\",\n",
    "}\n",
    "\n",
    "vitmatte_config = {\n",
    "    \"vit_b\": \"./configs/matte_anything.py\",\n",
    "}\n",
    "\n",
    "vitmatte_model = \"vit_b\"\n",
    "vitmatte = init_vitmatte(vitmatte_model)\n",
    "\n",
    "matte_anything_path = \"//home/fengkaige/codespace/Segment-and-Track-Anything\"\n",
    "os.chdir(matte_anything_path)\n",
    "sys.path.append(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: /home/fengkaige/codespace/Segment-and-Track-Anything/hugco/bert-base-uncased\n",
      "Model loaded from ./ckpt/groundingdino_swint_ogc.pth \n",
      " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "SegTracker has been initialized\n",
      "interactive_mask shape before bbox : (1080, 1920)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: BoxAnnotator is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     38\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame,cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m---> 39\u001b[0m pred_mask, annotated_frame \u001b[38;5;241m=\u001b[39m segtracker\u001b[38;5;241m.\u001b[39mdetect_and_seg_vitmatte(\n\u001b[1;32m     40\u001b[0m     frame, grounding_caption, box_threshold, text_threshold, box_size_threshold\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     44\u001b[0m obj_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(pred_mask)\n",
      "File \u001b[0;32m~/codespace/Segment-and-Track-Anything/SegTracker.py:343\u001b[0m, in \u001b[0;36mSegTracker.detect_and_seg_vitmatte\u001b[0;34m(self, origin_frame, grounding_caption, box_threshold, text_threshold, box_size_threshold, reset_image, run_vit_matte_func)\u001b[0m\n\u001b[1;32m    335\u001b[0m bbox_interactive_mask \u001b[38;5;241m=\u001b[39m interactive_mask[\n\u001b[1;32m    336\u001b[0m     bbox[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] : bbox[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], bbox[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] : bbox[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    337\u001b[0m ]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    339\u001b[0m bbox_img \u001b[38;5;241m=\u001b[39m origin_frame[\n\u001b[1;32m    340\u001b[0m     bbox[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] : bbox[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], bbox[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] : bbox[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    341\u001b[0m ]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 343\u001b[0m vitmatte_mask \u001b[38;5;241m=\u001b[39m run_vit_matte_func(bbox_img, bbox_interactive_mask)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# interactive_mask = run_vit_matte_func(origin_frame, interactive_mask)\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# interactive_mask = np.pad(\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m#     vitmatte_mask,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m#     constant_values=(0, 0),\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    355\u001b[0m interactive_mask[bbox[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] : bbox[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], bbox[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] : bbox[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    356\u001b[0m     vitmatte_mask\n\u001b[1;32m    357\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# choose good parameters in sam_args based on the first frame segmentation result\n",
    "# other arguments can be modified in model_args.py\n",
    "# note the object number limit is 255 by default, which requires < 10GB GPU memory with amp\n",
    "sam_args['generator_args'] = {\n",
    "        'points_per_side': 30,\n",
    "        'pred_iou_thresh': 0.8,\n",
    "        'stability_score_thresh': 0.9,\n",
    "        'crop_n_layers': 1,\n",
    "        'crop_n_points_downscale_factor': 2,\n",
    "        'min_mask_region_area': 200,\n",
    "    }\n",
    "\n",
    "# Set Text args\n",
    "'''\n",
    "parameter:\n",
    "    grounding_caption: Text prompt to detect objects in key-frames\n",
    "    box_threshold: threshold for box\n",
    "    text_threshold: threshold for label(text)\n",
    "    box_size_threshold: If the size ratio between the box and the frame is larger than the box_size_threshold, the box will be ignored. This is used to filter out large boxes.\n",
    "    reset_image: reset the image embeddings for SAM\n",
    "'''\n",
    "grounding_caption = \"car.suv\"\n",
    "# grounding_caption = \"door\"\n",
    "# grounding_caption = \"girl\"\n",
    "# grounding_caption = \"Beverage bottles\"\n",
    "# grounding_caption = \"\"\n",
    "# grounding_caption = \"cups\"\n",
    "# grounding_caption = \"left cups\"\n",
    "box_threshold, text_threshold, box_size_threshold, reset_image = 0.35, 0.5, 0.5, True\n",
    "\n",
    "cap = cv2.VideoCapture(io_args['input_video'])\n",
    "frame_idx = 0\n",
    "segtracker = SegTracker(segtracker_args,sam_args,aot_args)\n",
    "segtracker.restart_tracker()\n",
    "with torch.cuda.amp.autocast():\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        pred_mask, annotated_frame = segtracker.detect_and_seg(frame, grounding_caption, box_threshold, text_threshold, box_size_threshold)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        obj_ids = np.unique(pred_mask)\n",
    "        obj_ids = obj_ids[obj_ids != 0]\n",
    "        print(\n",
    "            \"processed frame {}, obj_num {}\".format(frame_idx, len(obj_ids)), end=\"\\n\"\n",
    "        )\n",
    "\n",
    "        # Vit Matte\n",
    "        # pred_mask = run_vit_matte(frame, pred_mask)\n",
    "\n",
    "        break\n",
    "    cap.release()\n",
    "    init_res = draw_mask(annotated_frame, pred_mask,id_countour=False)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(init_res)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(colorize_mask(pred_mask))\n",
    "    plt.show()\n",
    "\n",
    "    del segtracker\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate Results for the Whole Video\n",
    "# For every sam_gap frames, we use SAM to find new objects and add them for tracking\n",
    "# larger sam_gap is faster but may not spot new objects in time\n",
    "segtracker_args = {\n",
    "    'sam_gap': 49, # the interval to run sam to segment new objects\n",
    "    'min_area': 200, # minimal mask area to add a new mask as a new object\n",
    "    'max_obj_num': 255, # maximal object number to track in a video\n",
    "    'min_new_obj_iou': 0.8, # the area of a new object in the background should > 80%\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: /home/fengkaige/codespace/Segment-and-Track-Anything/hugco/bert-base-uncased\n",
      "Model loaded from ./ckpt/groundingdino_swint_ogc.pth \n",
      " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "SegTracker has been initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "SupervisionWarnings: BoxAnnotator is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed frame 48, obj_num 6\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: BoxAnnotator is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed frame 97, obj_num 6\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: BoxAnnotator is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed frame 146, obj_num 6\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: BoxAnnotator is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed frame 195, obj_num 6\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: BoxAnnotator is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed frame 229, obj_num 6\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "matte_anything_path = \"//home/fengkaige/codespace/Segment-and-Track-Anything\"\n",
    "os.chdir(matte_anything_path)\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "\n",
    "# source video to segment\n",
    "cap = cv2.VideoCapture(io_args['input_video'])\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "# output masks\n",
    "output_dir = io_args['output_mask_dir']\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "pred_list = []\n",
    "masked_pred_list = []\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "sam_gap = segtracker_args['sam_gap']\n",
    "frame_idx = 0\n",
    "segtracker = SegTracker(segtracker_args, sam_args, aot_args)\n",
    "segtracker.restart_tracker()\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        if frame_idx == 0:\n",
    "            # 第一帧：通过文字、框选、大小阈值等条件进行目标分割，得到预测mask\n",
    "            # 并将分割结果作为参考mask添加到跟踪器中\n",
    "            pred_mask, _ = segtracker.detect_and_seg(frame, grounding_caption, box_threshold, text_threshold, box_size_threshold, reset_image)\n",
    "            # pred_mask = cv2.imread('./debug/first_frame_mask.png', 0)\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            segtracker.add_reference(frame, pred_mask)\n",
    "        elif (frame_idx % sam_gap) == 0:\n",
    "            seg_mask, _ = segtracker.detect_and_seg(frame, grounding_caption, box_threshold, text_threshold, box_size_threshold, reset_image)\n",
    "            save_prediction(seg_mask, './debug/seg_result', str(frame_idx)+'.png')\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            track_mask = segtracker.track(frame)\n",
    "            save_prediction(track_mask, './debug/aot_result', str(frame_idx)+'.png')\n",
    "            # find new objects, and update tracker with new objects\n",
    "            new_obj_mask = segtracker.find_new_objs(track_mask, seg_mask)\n",
    "            if np.sum(new_obj_mask > 0) >  frame.shape[0] * frame.shape[1] * 0.4:\n",
    "                new_obj_mask = np.zeros_like(new_obj_mask)\n",
    "            save_prediction(new_obj_mask,output_dir,str(frame_idx)+'_new.png')\n",
    "            pred_mask = track_mask + new_obj_mask\n",
    "            # segtracker.restart_tracker()\n",
    "            segtracker.add_reference(frame, pred_mask)\n",
    "        else:\n",
    "            \"\"\"\n",
    "                frame:\n",
    "                    type: numpy.ndarray\n",
    "                    shape: (1080, 1920, 3)\n",
    "                    dtype: uint8\n",
    "            \"\"\"\n",
    "            pred_mask = segtracker.track(frame,update_memory=True)\n",
    "\n",
    "            '''\n",
    "            pred_mask\n",
    "                type: numpy.ndarray\n",
    "                shape : (1080, 1920)\n",
    "                dtype : uint8\n",
    "                min, max: 0, 1\n",
    "            '''\n",
    "            # mask -> trimap --(vit-matte)-->  alphe\n",
    "            # TODO\n",
    "            # output = run_vit_matte(frame, pred_mask)\n",
    "            # pred_mask = output\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        save_prediction(pred_mask,output_dir,str(frame_idx)+'.png')\n",
    "        # masked_frame = draw_mask(frame,pred_mask)\n",
    "        # masked_pred_list.append(masked_frame)\n",
    "        # plt.imshow(masked_frame)\n",
    "        # plt.show()\n",
    "\n",
    "        pred_list.append(pred_mask)\n",
    "\n",
    "\n",
    "        print(\"processed frame {}, obj_num {}\".format(frame_idx,segtracker.get_obj_num()),end='\\r')\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "    print('\\nfinished')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 229 writed\n",
      "./assets/423937009-1-208_seg.mp4 saved\n",
      "\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# draw pred mask on frame and save as a video\n",
    "cap = cv2.VideoCapture(io_args['input_video'])\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "if io_args['input_video'][-3:]=='mp4':\n",
    "    fourcc =  cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "elif io_args['input_video'][-3:] == 'avi':\n",
    "    fourcc =  cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "    # fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "else:\n",
    "    fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "out = cv2.VideoWriter(io_args['output_video'], fourcc, fps, (width, height))\n",
    "\n",
    "frame_idx = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "    pred_mask = pred_list[frame_idx]\n",
    "    masked_frame = draw_mask(frame,pred_mask)\n",
    "    # masked_frame = masked_pred_list[frame_idx]\n",
    "    masked_frame = cv2.cvtColor(masked_frame,cv2.COLOR_RGB2BGR)\n",
    "    out.write(masked_frame)\n",
    "    print('frame {} writed'.format(frame_idx),end='\\r')\n",
    "    frame_idx += 1\n",
    "out.release()\n",
    "cap.release()\n",
    "print(\"\\n{} saved\".format(io_args['output_video']))\n",
    "print('\\nfinished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1), <f2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/segmentAndTrackAnything_env/lib/python3.11/site-packages/PIL/Image.py:3130\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3130\u001b[0m     mode, rawmode \u001b[38;5;241m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[1;32m   3131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1), '<f2')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# save colorized masks as a gif\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m imageio\u001b[38;5;241m.\u001b[39mmimsave(io_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_gif\u001b[39m\u001b[38;5;124m'\u001b[39m],pred_list,fps\u001b[38;5;241m=\u001b[39mfps)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m saved\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(io_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_gif\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/miniconda3/envs/segmentAndTrackAnything_env/lib/python3.11/site-packages/imageio/v2.py:495\u001b[0m, in \u001b[0;36mmimwrite\u001b[0;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimopen_args) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mwrite(ims, is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/segmentAndTrackAnything_env/lib/python3.11/site-packages/imageio/plugins/pillow.py:444\u001b[0m, in \u001b[0;36mPillowPlugin.write\u001b[0;34m(self, ndimage, mode, format, is_batch, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m     ndimage \u001b[38;5;241m=\u001b[39m ndimage[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m ndimage:\n\u001b[0;32m--> 444\u001b[0m     pil_frame \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(frame, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbits\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    446\u001b[0m         pil_frame \u001b[38;5;241m=\u001b[39m pil_frame\u001b[38;5;241m.\u001b[39mquantize(colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbits\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/segmentAndTrackAnything_env/lib/python3.11/site-packages/PIL/Image.py:3134\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3132\u001b[0m         typekey_shape, typestr \u001b[38;5;241m=\u001b[39m typekey\n\u001b[1;32m   3133\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypekey_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypestr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3134\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   3135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3136\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1), <f2"
     ]
    }
   ],
   "source": [
    "# save colorized masks as a gif\n",
    "imageio.mimsave(io_args['output_gif'],pred_list,fps=fps)\n",
    "print(\"{} saved\".format(io_args['output_gif']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4857"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually release memory (after cuda out of memory)\n",
    "del segtracker\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('ldm': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "536611da043600e50719c9460971b5220bad26cd4a87e5994bfd4c9e9e5e7fb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
